{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O archive.zip \"https://storage.googleapis.com/kaggle-data-sets/500970/928025/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20210307%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210307T062240Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=56582821d50b586320ec4577bddf184342c308bed951e3611c9aa67e83261cbced21ed4c3cfc796682080b9690d9d9031570458a605730cf635b93bde57666da8a42bf48a1607f8c43a437ee8ce9af05e95005ff2659f6c3279f77f079d697fafed30fc6a0f2702550e2fc8cc3305d896f6fc8726003fd376999b10b62752e3e0f30981c814652384a708ae9a03fab3493edc0ba77fdf21b3ee2300d28ff45b676a8d688798f14e903f8beaa55ba7247bfa00cd806441508fc8eac9a011affcb816dc03a963b0d891efa16cfc711e34f341775a67e673ea1fe907fe401d4d496fe794f61509666d8bd5361d6aebf15b464b2db48c696e9f0b872eef4e86620ed\"\n",
    "\n",
    "!unzip -q archive.zip -d audio\n",
    "\n",
    "!rm archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('audio/UrbanSound8K.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4 # length (in seconds) of input\n",
    "desired_sr = 16000 # sampling rate to use\n",
    "mic_sr = 16000 # rate supported by Sampling library like PDM\n",
    "desired_samples = max_length*desired_sr # total number of samples in input\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n",
    "os.environ['PYTHONHASHSEED'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the data\n",
    "dataset_min = 0.0\n",
    "dataset_max = 1.0\n",
    "\n",
    "def denormalize_dataset(input_val):\n",
    "  global dataset_min, dataset_max\n",
    "  return input_val * (dataset_max - dataset_min)\n",
    "\n",
    "#Function to normalize input values\n",
    "def normalize_dataset(input_val):\n",
    "  global dataset_min, dataset_max\n",
    "  dataset_min = np.min(input_val) \n",
    "  dataset_max = np.max(input_val) \n",
    "\n",
    "  diff = dataset_max - dataset_min\n",
    "  if (diff != 0):\n",
    "    input_val /= diff\n",
    "  return input_val\n",
    "\n",
    "def interpolateAudio(audio):\n",
    "    factor = float(mic_sr)/desired_sr\n",
    "    x_interp_values = []\n",
    "    for i in range(len(audio)):\n",
    "        x_interp_values.append(int(factor*i))\n",
    "    audio_interpolated = np.interp(range(int(len(audio)*factor)), x_interp_values, audio)\n",
    "\n",
    "    return mic_sr, audio_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dirs = list(set(df['class'].to_list()))\n",
    "hotwords = ['jackhammer', 'dog_bark', 'siren', 'gun_shot']\n",
    "\n",
    "print(\"All words in dataset - \\n\", ', '.join(word_dirs))\n",
    "print(\"\\nHotwords - \\n\", ', '.join(hotwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise = False # add different words, null samples and random noise\n",
    "n_classes = len(hotwords) + int(add_noise) \n",
    "\n",
    "class_nSamples = 1000\n",
    "other_nSamples = float(class_nSamples)/(len(word_dirs) - n_classes)\n",
    "\n",
    "def nLabel(word):\n",
    "    return n_classes-1 if ( word not in hotwords ) else hotwords.index(word)\n",
    "\n",
    "def textLabel(index):\n",
    "    return hotwords[index] if index <len(hotwords) else \"background\"\n",
    "\n",
    "def sampleBackGround():\n",
    "    return add_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset storing audio samples for wake word and background\n",
    "\n",
    "top_dir = \"audio\"\n",
    "\n",
    "input_audio   = np.empty((0, desired_samples)).astype(np.float32)\n",
    "input_labels  = np.empty((0)).astype(np.int32); # index of the word in hotwords list is the lable.\n",
    "\n",
    "for word in (word_dirs) :\n",
    "    print(\"\\n\",word)\n",
    "    \n",
    "    if ( word not in hotwords and False == sampleBackGround()) : # background, do not include\n",
    "        print(\"-- Background/noise/other words not included\")\n",
    "        continue\n",
    "        \n",
    "    else: # to be included\n",
    "        dfx = df[df['class'] == word]\n",
    "        start_time = time.time()\n",
    "\n",
    "        wav_files = 0\n",
    "\n",
    "        word_samples = np.empty((0, desired_samples))\n",
    "        \n",
    "        if word in hotwords: # hotwords\n",
    "            print(\"-- Category : hotword\")\n",
    "            \n",
    "            for i in range(len(dfx)):\n",
    "                file_path = top_dir + \"/fold\" + str(dfx.iloc[i]['fold']) + \"/\" + str(dfx.iloc[i]['slice_file_name'])\n",
    "\n",
    "                X_sub = np.empty((0, desired_samples))\n",
    "                X, sr = librosa.core.load(file_path, sr=desired_sr)\n",
    "                X, interval = librosa.effects.trim(X)\n",
    "\n",
    "                if X.shape[0] < desired_sr: # if samples less than 1 second\n",
    "                    continue\n",
    "\n",
    "                if X.shape[0]%desired_samples != 0: # if it needs padding, else, there will be unnecessary silence appended\n",
    "                    X = np.pad(X, (0, desired_samples - (X.shape[0]%desired_samples)))\n",
    "                \n",
    "                X_sub = np.array(np.split(X, int(X.shape[0]*1.0/desired_samples)))\n",
    "                \n",
    "                word_samples = np.append(word_samples, X_sub, axis=0)\n",
    "\n",
    "                if ( word_samples.shape[0] > class_nSamples ):\n",
    "                    break\n",
    "\n",
    "                wav_files = wav_files + 1\n",
    "            \n",
    "        else:\n",
    "            print(\"-- Category : backgound/noise/other words\")\n",
    "\n",
    "            for i in range(len(dfx)):\n",
    "                file_path = top_dir + \"/fold\" + str(dfx.iloc[i]['fold']) + \"/\" + str(dfx.iloc[i]['slice_file_name'])\n",
    "\n",
    "                X, sr = librosa.core.load(file_path, sr=desired_sr)\n",
    "                X, interval = librosa.effects.trim(X)\n",
    "                X = np.pad(X, (0,desired_samples - (X.shape[0]%desired_samples)))\n",
    "                X_sub = np.array(np.split(X, int(X.shape[0]*1.0/desired_samples)))\n",
    "\n",
    "                word_samples = np.append(word_samples, X_sub, axis=0)\n",
    "\n",
    "                if ( word_samples.shape[0] > other_nSamples ):\n",
    "                    break\n",
    "                \n",
    "                wav_files = wav_files + 1\n",
    "            \n",
    "        if ( word_samples.size > 0 ):\n",
    "            input_audio = np.concatenate((input_audio, word_samples), axis=0)\n",
    "            labels = np.full((word_samples.shape[0]), nLabel(word))\n",
    "            input_labels = np.concatenate((input_labels, labels))\n",
    "\n",
    "            print(\"added {} audio files with {} samples for word \\\"{}\\\" with label {} in {:.1f} sec.\".\n",
    "                  format(wav_files, labels.shape[0], word, nLabel(word), (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating dataset into matrix of inputs and labels\n",
    "\n",
    "onehot_labels = np.zeros((input_labels.size, n_classes)).astype(np.int32)\n",
    "onehot_labels[np.arange(input_labels.size), input_labels] = 1\n",
    "\n",
    "input_labels = onehot_labels\n",
    "print(\"Input dataset size:\", input_audio.shape)\n",
    "print(\"Input targets size:\", input_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 10% of random noise and 10% of silent samples as background.\n",
    "if ( sampleBackGround() ) :\n",
    "    n_bg_samples = int(other_nSamples)\n",
    "\n",
    "    bg_labels    = np.zeros((n_bg_samples, n_classes)).astype(np.int)\n",
    "    bg_labels[:,n_classes-1] = 1\n",
    "\n",
    "    silence = np.zeros((n_bg_samples, desired_samples))\n",
    "    input_audio = np.append(input_audio, silence, axis=0)\n",
    "    input_labels = np.append(input_labels, bg_labels, axis=0)\n",
    "    \n",
    "    background = np.zeros((n_bg_samples, desired_samples))\n",
    "    input_audio = np.append(input_audio, background, axis=0)\n",
    "    input_labels = np.append(input_labels, bg_labels, axis=0)\n",
    "\n",
    "#     %xdel background\n",
    "#     %xdel silence\n",
    "#     %xdel bg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hop_len=int(win_len/4) # default\n",
    "# fft_len=pow(2, int(np.log2(win_len)+1))\n",
    "fft_len = 2048\n",
    "win_len = fft_len\n",
    "hop_len = int(win_len/4)\n",
    "\n",
    "def spectrogramOp(X):\n",
    "  # STFT returns np.ndarray of shape=(1 + fft_len/2, t)\n",
    "  spectrogram_out = librosa.core.stft(X, n_fft=fft_len, hop_length=hop_len, win_length=win_len, center=True)\n",
    "#  spectrogram_out = np.swapaxes(np.abs(spectrogram_out), 0, 1)\n",
    "  return np.absolute(spectrogram_out)\n",
    "\n",
    "#inputs = np.array([spectrogramOp(input) for input in input_audio])\n",
    "input_spectrogram = np.empty((input_audio.shape[0], int(fft_len/2 + 1), int(desired_samples/hop_len + 1))).astype(np.float32)\n",
    "\n",
    "i = 0 ;\n",
    "for input in input_audio:\n",
    "    input_spectrogram[i] = spectrogramOp(input) \n",
    "    i = i +  1\n",
    "print(\"input dataset size:\", input_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "input_spectrogram = normalize_dataset(input_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = input_labels.shape[0]\n",
    "\n",
    "#Shuffling inputs and labels\n",
    "shuffle_permutation = np.arange(total_len)\n",
    "np.random.shuffle(shuffle_permutation)\n",
    "\n",
    "input_spectrogram = input_spectrogram[shuffle_permutation]\n",
    "input_labels = input_labels[shuffle_permutation]\n",
    "\n",
    "#Splitting into train and test dataset - 90-10 ratio\n",
    "train_split = 0.9\n",
    "cutoff = int(train_split*total_len)\n",
    "\n",
    "inputs_train = input_spectrogram[:cutoff]\n",
    "inputs_test = input_spectrogram[cutoff:]\n",
    "labels_train = input_labels[:cutoff]\n",
    "labels_test = input_labels[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting random index from test dataset\n",
    "\n",
    "ind = int(np.random.uniform()*len(inputs_train))\n",
    "\n",
    "#Displaying sample spectrogram and audio from test dataset\n",
    "X = inputs_train[ind]\n",
    "y = labels_train[ind].argmax()\n",
    "print(\"Label :\", textLabel(y) )\n",
    "plt.imshow(X, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()\n",
    "audio = librosa.core.istft(X, hop_length=hop_len, win_length=win_len)\n",
    "\n",
    "ipd.Audio(audio, rate=desired_sr, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# (-1, 126, 65)  = inputs_train.shape. Replace the numbers from inputs_train, if it changes.\n",
    "# 126 = int(desired_samples/hop_len + 1), = time axis\n",
    "# 65  = int(fft_len/2 + 1) = freq bins\n",
    "\n",
    "lambda1 = tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, int(fft_len/2 + 1), int(desired_samples/hop_len + 1), 1)), \n",
    "                                 name=\"add_channels\", input_shape=(None, int(fft_len/2 + 1), int(desired_samples/hop_len + 1)))\n",
    "conv2d1 = tf.keras.layers.Conv2D(16, (int(fft_len/2 + 1), 4), strides=1, activation='relu', name=\"conv1\", \n",
    "                                 input_shape=(int(fft_len/2 + 1), int(desired_samples/hop_len + 1), 1))\n",
    "conv2d2 = tf.keras.layers.Conv2D(32, (1, 4), strides=4, activation='relu', name=\"conv2\")\n",
    "conv2d3 = tf.keras.layers.Conv2D(64, (1, 4), strides=3, activation='relu', name=\"conv3\")\n",
    "flatten1 = tf.keras.layers.Flatten()\n",
    "dense1  = tf.keras.layers.Dense(6*n_classes)\n",
    "dense2  = tf.keras.layers.Dense(n_classes)\n",
    "dropout = tf.keras.layers.Dropout(0.2)\n",
    "activation1 = tf.keras.layers.Activation('softmax')\n",
    "\n",
    "model.add(lambda1)\n",
    "model.add(conv2d1)\n",
    "model.add(conv2d2)\n",
    "model.add(conv2d3)\n",
    "model.add(flatten1)\n",
    "model.add(dense1)\n",
    "model.add(dense2) \n",
    "model.add(dropout)\n",
    "model.add(activation1)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'],\n",
    "              loss=(tf.keras.losses.binary_crossentropy if (n_classes==2) else tf.keras.losses.categorical_crossentropy))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs_train, labels_train, batch_size=64, epochs=1024, callbacks=callbacks, validation_data=(inputs_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = int(np.random.uniform()*len(inputs_test))\n",
    "spectrogram_out = inputs_test[ind]\n",
    "\n",
    "ipd.Audio(spectrogram_out, rate=desired_sr)\n",
    "\n",
    "y = labels_test[ind]\n",
    "output = model.predict(np.expand_dims(np.array([inputs_test[ind]]), 0))\n",
    "\n",
    "print(\"True label:\", textLabel(np.argmax(y)))\n",
    "print(\"Prediction:\", textLabel(np.argmax(output)))\n",
    "\n",
    "plt.imshow(spectrogram_out, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Playing the sample\n",
    "'''\n",
    "audio = librosa.core.istft(spectrogram_out, hop_length=hop_len, win_length=win_len)\n",
    "playback_sr, audio_interpolated = interpolateAudio(audio)\n",
    "ipd.Audio(audio_interpolated, rate=playback_sr, autoplay=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = spectrogramOp(np.random.random((desired_samples)))\n",
    "silence = spectrogramOp(np.zeros((desired_samples)))\n",
    "background_out, silence_out = model.predict(np.array([background, silence]))\n",
    "print(\"Predicted \", textLabel(background_out.argmax()), \"on random audio with vector\", background_out)\n",
    "print(\"Predicted \", textLabel(silence_out.argmax()), \"on null audio with vector\", silence_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
