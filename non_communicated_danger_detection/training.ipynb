{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for data.\n",
    "path = \"/AudioWAV/\" # not on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_directory_list = os.listdir(path)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in path_directory_list:\n",
    "    # storing file paths\n",
    "    file_path.append(path + file)\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "ata_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the data\n",
    "dataset_min = 0.0\n",
    "dataset_max = 1.0\n",
    "\n",
    "def denormalize_dataset(input_val):\n",
    "  global dataset_min, dataset_max\n",
    "  return input_val * (dataset_max - dataset_min)\n",
    "\n",
    "#Function to normalize input values\n",
    "def normalize_dataset(input_val):\n",
    "  global dataset_min, dataset_max\n",
    "  dataset_min = np.min(input_val) \n",
    "  dataset_max = np.max(input_val) \n",
    "\n",
    "  diff = dataset_max - dataset_min\n",
    "  if (diff != 0):\n",
    "    input_val /= diff\n",
    "  return input_val\n",
    "\n",
    "def interpolateAudio(audio):\n",
    "    factor = float(mic_sr)/desired_sr\n",
    "    x_interp_values = []\n",
    "    for i in range(len(audio)):\n",
    "        x_interp_values.append(int(factor*i))\n",
    "    audio_interpolated = np.interp(range(int(len(audio)*factor)), x_interp_values, audio)\n",
    "\n",
    "    return mic_sr, audio_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dirs = list(set(data_df['class'].to_list()))\n",
    "hotwords = path_directory_list\n",
    "\n",
    "print(\"All words in dataset - \\n\", ', '.join(word_dirs))\n",
    "print(\"\\nHotwords - \\n\", ', '.join(hotwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise = False # add different words, null samples and random noise\n",
    "n_classes = len(hotwords) + int(add_noise) \n",
    "\n",
    "class_nSamples = 1000\n",
    "other_nSamples = float(class_nSamples)/(len(word_dirs) - n_classes)\n",
    "\n",
    "def nLabel(word):\n",
    "    return n_classes-1 if ( word not in hotwords ) else hotwords.index(word)\n",
    "\n",
    "def textLabel(index):\n",
    "    return hotwords[index] if index <len(hotwords) else \"background\"\n",
    "\n",
    "def sampleBackGround():\n",
    "    return add_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset storing audio samples for wake word and background\n",
    "\n",
    "top_dir = \"audio\"\n",
    "\n",
    "input_audio   = np.empty((0, desired_samples)).astype(np.float32)\n",
    "input_labels  = np.empty((0)).astype(np.int32); # index of the word in hotwords list is the lable.\n",
    "\n",
    "for word in (word_dirs) :\n",
    "    print(\"\\n\",word)\n",
    "    \n",
    "    if ( word not in hotwords and False == sampleBackGround()) : # background, do not include\n",
    "        print(\"-- Background/noise/other words not included\")\n",
    "        continue\n",
    "        \n",
    "    else: # to be included\n",
    "        dfx = data_df[data_df['class'] == word]\n",
    "        start_time = time.time()\n",
    "\n",
    "        wav_files = 0\n",
    "\n",
    "        word_samples = np.empty((0, desired_samples))\n",
    "        \n",
    "        if word in hotwords: # hotwords\n",
    "            print(\"-- Category : hotword\")\n",
    "            \n",
    "            for i in range(len(dfx)):\n",
    "                file_path = top_dir + \"/fold\" + str(dfx.iloc[i]['fold']) + \"/\" + str(dfx.iloc[i]['slice_file_name'])\n",
    "\n",
    "                X_sub = np.empty((0, desired_samples))\n",
    "                X, sr = librosa.core.load(file_path, sr=desired_sr)\n",
    "                X, interval = librosa.effects.trim(X)\n",
    "\n",
    "                if X.shape[0] < desired_sr: # if samples less than 1 second\n",
    "                    continue\n",
    "\n",
    "                if X.shape[0]%desired_samples != 0: # if it needs padding, else, there will be unnecessary silence appended\n",
    "                    X = np.pad(X, (0, desired_samples - (X.shape[0]%desired_samples)))\n",
    "                \n",
    "                X_sub = np.array(np.split(X, int(X.shape[0]*1.0/desired_samples)))\n",
    "                \n",
    "                word_samples = np.append(word_samples, X_sub, axis=0)\n",
    "\n",
    "                if ( word_samples.shape[0] > class_nSamples ):\n",
    "                    break\n",
    "\n",
    "                wav_files = wav_files + 1\n",
    "            \n",
    "        else:\n",
    "            print(\"-- Category : backgound/noise/other words\")\n",
    "\n",
    "            for i in range(len(dfx)):\n",
    "                file_path = top_dir + \"/fold\" + str(dfx.iloc[i]['fold']) + \"/\" + str(dfx.iloc[i]['slice_file_name'])\n",
    "\n",
    "                X, sr = librosa.core.load(file_path, sr=desired_sr)\n",
    "                X, interval = librosa.effects.trim(X)\n",
    "                X = np.pad(X, (0,desired_samples - (X.shape[0]%desired_samples)))\n",
    "                X_sub = np.array(np.split(X, int(X.shape[0]*1.0/desired_samples)))\n",
    "\n",
    "                word_samples = np.append(word_samples, X_sub, axis=0)\n",
    "\n",
    "                if ( word_samples.shape[0] > other_nSamples ):\n",
    "                    break\n",
    "                \n",
    "                wav_files = wav_files + 1\n",
    "            \n",
    "        if ( word_samples.size > 0 ):\n",
    "            input_audio = np.concatenate((input_audio, word_samples), axis=0)\n",
    "            labels = np.full((word_samples.shape[0]), nLabel(word))\n",
    "            input_labels = np.concatenate((input_labels, labels))\n",
    "\n",
    "            print(\"added {} audio files with {} samples for word \\\"{}\\\" with label {} in {:.1f} sec.\".\n",
    "                  format(wav_files, labels.shape[0], word, nLabel(word), (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating dataset into matrix of inputs and labels\n",
    "\n",
    "onehot_labels = np.zeros((input_labels.size, n_classes)).astype(np.int32)\n",
    "onehot_labels[np.arange(input_labels.size), input_labels] = 1\n",
    "\n",
    "input_labels = onehot_labels\n",
    "print(\"Input dataset size:\", input_audio.shape)\n",
    "print(\"Input targets size:\", input_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 10% of random noise and 10% of silent samples as background.\n",
    "if ( sampleBackGround() ) :\n",
    "    n_bg_samples = int(other_nSamples)\n",
    "\n",
    "    bg_labels    = np.zeros((n_bg_samples, n_classes)).astype(np.int)\n",
    "    bg_labels[:,n_classes-1] = 1\n",
    "\n",
    "    silence = np.zeros((n_bg_samples, desired_samples))\n",
    "    input_audio = np.append(input_audio, silence, axis=0)\n",
    "    input_labels = np.append(input_labels, bg_labels, axis=0)\n",
    "    \n",
    "    background = np.zeros((n_bg_samples, desired_samples))\n",
    "    input_audio = np.append(input_audio, background, axis=0)\n",
    "    input_labels = np.append(input_labels, bg_labels, axis=0)\n",
    "\n",
    "#     %xdel background\n",
    "#     %xdel silence\n",
    "#     %xdel bg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hop_len=int(win_len/4) # default\n",
    "# fft_len=pow(2, int(np.log2(win_len)+1))\n",
    "fft_len = 2048\n",
    "win_len = fft_len\n",
    "hop_len = int(win_len/4)\n",
    "\n",
    "def spectrogramOp(X):\n",
    "  # STFT returns np.ndarray of shape=(1 + fft_len/2, t)\n",
    "  spectrogram_out = librosa.core.stft(X, n_fft=fft_len, hop_length=hop_len, win_length=win_len, center=True)\n",
    "#  spectrogram_out = np.swapaxes(np.abs(spectrogram_out), 0, 1)\n",
    "  return np.absolute(spectrogram_out)\n",
    "\n",
    "#inputs = np.array([spectrogramOp(input) for input in input_audio])\n",
    "input_spectrogram = np.empty((input_audio.shape[0], int(fft_len/2 + 1), int(desired_samples/hop_len + 1))).astype(np.float32)\n",
    "\n",
    "i = 0 ;\n",
    "for input in input_audio:\n",
    "    input_spectrogram[i] = spectrogramOp(input) \n",
    "    i = i +  1\n",
    "print(\"input dataset size:\", input_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "input_spectrogram = normalize_dataset(input_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = input_labels.shape[0]\n",
    "\n",
    "#Shuffling inputs and labels\n",
    "shuffle_permutation = np.arange(total_len)\n",
    "np.random.shuffle(shuffle_permutation)\n",
    "\n",
    "input_spectrogram = input_spectrogram[shuffle_permutation]\n",
    "input_labels = input_labels[shuffle_permutation]\n",
    "\n",
    "#Splitting into train and test dataset - 90-10 ratio\n",
    "train_split = 0.9\n",
    "cutoff = int(train_split*total_len)\n",
    "\n",
    "inputs_train = input_spectrogram[:cutoff]\n",
    "inputs_test = input_spectrogram[cutoff:]\n",
    "labels_train = input_labels[:cutoff]\n",
    "labels_test = input_labels[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting random index from test dataset\n",
    "\n",
    "ind = int(np.random.uniform()*len(inputs_train))\n",
    "\n",
    "#Displaying sample spectrogram and audio from test dataset\n",
    "X = inputs_train[ind]\n",
    "y = labels_train[ind].argmax()\n",
    "print(\"Label :\", textLabel(y) )\n",
    "plt.imshow(X, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()\n",
    "audio = librosa.core.istft(X, hop_length=hop_len, win_length=win_len)\n",
    "\n",
    "ipd.Audio(audio, rate=desired_sr, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# (-1, 126, 65)  = inputs_train.shape. Replace the numbers from inputs_train, if it changes.\n",
    "# 126 = int(desired_samples/hop_len + 1), = time axis\n",
    "# 65  = int(fft_len/2 + 1) = freq bins\n",
    "\n",
    "lambda1 = tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, int(fft_len/2 + 1), int(desired_samples/hop_len + 1), 1)), \n",
    "                                 name=\"add_channels\", input_shape=(None, int(fft_len/2 + 1), int(desired_samples/hop_len + 1)))\n",
    "conv2d1 = tf.keras.layers.Conv2D(16, (int(fft_len/2 + 1), 4), strides=1, activation='relu', name=\"conv1\", \n",
    "                                 input_shape=(int(fft_len/2 + 1), int(desired_samples/hop_len + 1), 1))\n",
    "conv2d2 = tf.keras.layers.Conv2D(32, (1, 4), strides=4, activation='relu', name=\"conv2\")\n",
    "conv2d3 = tf.keras.layers.Conv2D(64, (1, 4), strides=3, activation='relu', name=\"conv3\")\n",
    "flatten1 = tf.keras.layers.Flatten()\n",
    "dense1  = tf.keras.layers.Dense(6*n_classes)\n",
    "dense2  = tf.keras.layers.Dense(n_classes)\n",
    "dropout = tf.keras.layers.Dropout(0.2)\n",
    "activation1 = tf.keras.layers.Activation('softmax')\n",
    "\n",
    "model.add(lambda1)\n",
    "model.add(conv2d1)\n",
    "model.add(conv2d2)\n",
    "model.add(conv2d3)\n",
    "model.add(flatten1)\n",
    "model.add(dense1)\n",
    "model.add(dense2) \n",
    "model.add(dropout)\n",
    "model.add(activation1)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'],\n",
    "              loss=(tf.keras.losses.binary_crossentropy if (n_classes==2) else tf.keras.losses.categorical_crossentropy))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs_train, labels_train, batch_size=64, epochs=1024, callbacks=callbacks, validation_data=(inputs_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = int(np.random.uniform()*len(inputs_test))\n",
    "spectrogram_out = inputs_test[ind]\n",
    "\n",
    "ipd.Audio(spectrogram_out, rate=desired_sr)\n",
    "\n",
    "y = labels_test[ind]\n",
    "output = model.predict(np.expand_dims(np.array([inputs_test[ind]]), 0))\n",
    "\n",
    "print(\"True label:\", textLabel(np.argmax(y)))\n",
    "print(\"Prediction:\", textLabel(np.argmax(output)))\n",
    "\n",
    "plt.imshow(spectrogram_out, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = spectrogramOp(np.random.random((desired_samples)))\n",
    "silence = spectrogramOp(np.zeros((desired_samples)))\n",
    "background_out, silence_out = model.predict(np.array([background, silence]))\n",
    "print(\"Predicted \", textLabel(background_out.argmax()), \"on random audio with vector\", background_out)\n",
    "print(\"Predicted \", textLabel(silence_out.argmax()), \"on null audio with vector\", silence_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
