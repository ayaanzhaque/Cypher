{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for data.\n",
    "path = \"/AudioWAV/\" # not on Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_directory_list = os.listdir(path)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in path_directory_list:\n",
    "    # storing file paths\n",
    "    file_path.append(path + file)\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "ata_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the data\n",
    "dataset_min = 0.0\n",
    "dataset_max = 1.0\n",
    "\n",
    "def denormalize_dataset(input_val):\n",
    "  global dataset_min, dataset_max\n",
    "  return input_val * (dataset_max - dataset_min)\n",
    "\n",
    "#Function to normalize input values\n",
    "def normalize_dataset(input_val):\n",
    "  global dataset_min, dataset_max\n",
    "  dataset_min = np.min(input_val) \n",
    "  dataset_max = np.max(input_val) \n",
    "\n",
    "  diff = dataset_max - dataset_min\n",
    "  if (diff != 0):\n",
    "    input_val /= diff\n",
    "  return input_val\n",
    "\n",
    "def interpolateAudio(audio):\n",
    "    factor = float(mic_sr)/desired_sr\n",
    "    x_interp_values = []\n",
    "    for i in range(len(audio)):\n",
    "        x_interp_values.append(int(factor*i))\n",
    "    audio_interpolated = np.interp(range(int(len(audio)*factor)), x_interp_values, audio)\n",
    "\n",
    "    return mic_sr, audio_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dirs = list(set(data_df['class'].to_list()))\n",
    "hotwords = path_directory_list\n",
    "\n",
    "print(\"All words in dataset - \\n\", ', '.join(word_dirs))\n",
    "print(\"\\nHotwords - \\n\", ', '.join(hotwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise = False # add different words, null samples and random noise\n",
    "n_classes = len(hotwords) + int(add_noise) \n",
    "\n",
    "class_nSamples = 1000\n",
    "other_nSamples = float(class_nSamples)/(len(word_dirs) - n_classes)\n",
    "\n",
    "def nLabel(word):\n",
    "    return n_classes-1 if ( word not in hotwords ) else hotwords.index(word)\n",
    "\n",
    "def textLabel(index):\n",
    "    return hotwords[index] if index <len(hotwords) else \"background\"\n",
    "\n",
    "def sampleBackGround():\n",
    "    return add_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset storing audio samples for wake word and background\n",
    "\n",
    "top_dir = \"audio\"\n",
    "\n",
    "input_audio   = np.empty((0, desired_samples)).astype(np.float32)\n",
    "input_labels  = np.empty((0)).astype(np.int32); # index of the word in hotwords list is the lable.\n",
    "\n",
    "for word in (word_dirs) :\n",
    "    print(\"\\n\",word)\n",
    "    \n",
    "    if ( word not in hotwords and False == sampleBackGround()) : # background, do not include\n",
    "        print(\"-- Background/noise/other words not included\")\n",
    "        continue\n",
    "        \n",
    "    else: # to be included\n",
    "        dfx = data_df[data_df['class'] == word]\n",
    "        start_time = time.time()\n",
    "\n",
    "        wav_files = 0\n",
    "\n",
    "        word_samples = np.empty((0, desired_samples))\n",
    "        \n",
    "        if word in hotwords: # hotwords\n",
    "            print(\"-- Category : hotword\")\n",
    "            \n",
    "            for i in range(len(dfx)):\n",
    "                file_path = top_dir + \"/fold\" + str(dfx.iloc[i]['fold']) + \"/\" + str(dfx.iloc[i]['slice_file_name'])\n",
    "\n",
    "                X_sub = np.empty((0, desired_samples))\n",
    "                X, sr = librosa.core.load(file_path, sr=desired_sr)\n",
    "                X, interval = librosa.effects.trim(X)\n",
    "\n",
    "                if X.shape[0] < desired_sr: # if samples less than 1 second\n",
    "                    continue\n",
    "\n",
    "                if X.shape[0]%desired_samples != 0: # if it needs padding, else, there will be unnecessary silence appended\n",
    "                    X = np.pad(X, (0, desired_samples - (X.shape[0]%desired_samples)))\n",
    "                \n",
    "                X_sub = np.array(np.split(X, int(X.shape[0]*1.0/desired_samples)))\n",
    "                \n",
    "                word_samples = np.append(word_samples, X_sub, axis=0)\n",
    "\n",
    "                if ( word_samples.shape[0] > class_nSamples ):\n",
    "                    break\n",
    "\n",
    "                wav_files = wav_files + 1\n",
    "            \n",
    "        else:\n",
    "            print(\"-- Category : backgound/noise/other words\")\n",
    "\n",
    "            for i in range(len(dfx)):\n",
    "                file_path = top_dir + \"/fold\" + str(dfx.iloc[i]['fold']) + \"/\" + str(dfx.iloc[i]['slice_file_name'])\n",
    "\n",
    "                X, sr = librosa.core.load(file_path, sr=desired_sr)\n",
    "                X, interval = librosa.effects.trim(X)\n",
    "                X = np.pad(X, (0,desired_samples - (X.shape[0]%desired_samples)))\n",
    "                X_sub = np.array(np.split(X, int(X.shape[0]*1.0/desired_samples)))\n",
    "\n",
    "                word_samples = np.append(word_samples, X_sub, axis=0)\n",
    "\n",
    "                if ( word_samples.shape[0] > other_nSamples ):\n",
    "                    break\n",
    "                \n",
    "                wav_files = wav_files + 1\n",
    "            \n",
    "        if ( word_samples.size > 0 ):\n",
    "            input_audio = np.concatenate((input_audio, word_samples), axis=0)\n",
    "            labels = np.full((word_samples.shape[0]), nLabel(word))\n",
    "            input_labels = np.concatenate((input_labels, labels))\n",
    "\n",
    "            print(\"added {} audio files with {} samples for word \\\"{}\\\" with label {} in {:.1f} sec.\".\n",
    "                  format(wav_files, labels.shape[0], word, nLabel(word), (time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating dataset into matrix of inputs and labels\n",
    "\n",
    "onehot_labels = np.zeros((input_labels.size, n_classes)).astype(np.int32)\n",
    "onehot_labels[np.arange(input_labels.size), input_labels] = 1\n",
    "\n",
    "input_labels = onehot_labels\n",
    "print(\"Input dataset size:\", input_audio.shape)\n",
    "print(\"Input targets size:\", input_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 10% of random noise and 10% of silent samples as background.\n",
    "if ( sampleBackGround() ) :\n",
    "    n_bg_samples = int(other_nSamples)\n",
    "\n",
    "    bg_labels    = np.zeros((n_bg_samples, n_classes)).astype(np.int)\n",
    "    bg_labels[:,n_classes-1] = 1\n",
    "\n",
    "    silence = np.zeros((n_bg_samples, desired_samples))\n",
    "    input_audio = np.append(input_audio, silence, axis=0)\n",
    "    input_labels = np.append(input_labels, bg_labels, axis=0)\n",
    "    \n",
    "    background = np.zeros((n_bg_samples, desired_samples))\n",
    "    input_audio = np.append(input_audio, background, axis=0)\n",
    "    input_labels = np.append(input_labels, bg_labels, axis=0)\n",
    "\n",
    "#     %xdel background\n",
    "#     %xdel silence\n",
    "#     %xdel bg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hop_len=int(win_len/4) # default\n",
    "# fft_len=pow(2, int(np.log2(win_len)+1))\n",
    "fft_len = 2048\n",
    "win_len = fft_len\n",
    "hop_len = int(win_len/4)\n",
    "\n",
    "def spectrogramOp(X):\n",
    "  # STFT returns np.ndarray of shape=(1 + fft_len/2, t)\n",
    "  spectrogram_out = librosa.core.stft(X, n_fft=fft_len, hop_length=hop_len, win_length=win_len, center=True)\n",
    "#  spectrogram_out = np.swapaxes(np.abs(spectrogram_out), 0, 1)\n",
    "  return np.absolute(spectrogram_out)\n",
    "\n",
    "#inputs = np.array([spectrogramOp(input) for input in input_audio])\n",
    "input_spectrogram = np.empty((input_audio.shape[0], int(fft_len/2 + 1), int(desired_samples/hop_len + 1))).astype(np.float32)\n",
    "\n",
    "i = 0 ;\n",
    "for input in input_audio:\n",
    "    input_spectrogram[i] = spectrogramOp(input) \n",
    "    i = i +  1\n",
    "print(\"input dataset size:\", input_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "input_spectrogram = normalize_dataset(input_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = input_labels.shape[0]\n",
    "\n",
    "#Shuffling inputs and labels\n",
    "shuffle_permutation = np.arange(total_len)\n",
    "np.random.shuffle(shuffle_permutation)\n",
    "\n",
    "input_spectrogram = input_spectrogram[shuffle_permutation]\n",
    "input_labels = input_labels[shuffle_permutation]\n",
    "\n",
    "#Splitting into train and test dataset - 90-10 ratio\n",
    "train_split = 0.9\n",
    "cutoff = int(train_split*total_len)\n",
    "\n",
    "inputs_train = input_spectrogram[:cutoff]\n",
    "inputs_test = input_spectrogram[cutoff:]\n",
    "labels_train = input_labels[:cutoff]\n",
    "labels_test = input_labels[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting random index from test dataset\n",
    "\n",
    "ind = int(np.random.uniform()*len(inputs_train))\n",
    "\n",
    "#Displaying sample spectrogram and audio from test dataset\n",
    "X = inputs_train[ind]\n",
    "y = labels_train[ind].argmax()\n",
    "print(\"Label :\", textLabel(y) )\n",
    "plt.imshow(X, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()\n",
    "audio = librosa.core.istft(X, hop_length=hop_len, win_length=win_len)\n",
    "\n",
    "ipd.Audio(audio, rate=desired_sr, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras import backend as K\n",
    "\n",
    "import vggish_params as params\n",
    "\n",
    "def VGGish(load_weights=True, weights='audioset',\n",
    "           input_tensor=None, input_shape=None,\n",
    "           out_dim=None, include_top=True, pooling='avg'):\n",
    "    '''\n",
    "    An implementation of the VGGish architecture.\n",
    "\n",
    "    :param load_weights: if load weights\n",
    "    :param weights: loads weights pre-trained on a preliminary version of YouTube-8M.\n",
    "    :param input_tensor: input_layer\n",
    "    :param input_shape: input data shape\n",
    "    :param out_dim: output dimension\n",
    "    :param include_top:whether to include the 3 fully-connected layers at the top of the network.\n",
    "    :param pooling: pooling type over the non-top network, 'avg' or 'max'\n",
    "\n",
    "    :return: A Keras model instance.\n",
    "    '''\n",
    "\n",
    "    if weights not in {'audioset', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `audioset` '\n",
    "                         '(pre-training on audioset).')\n",
    "\n",
    "    if out_dim is None:\n",
    "        out_dim = params.EMBEDDING_SIZE\n",
    "\n",
    "    # input shape\n",
    "    if input_shape is None:\n",
    "        input_shape = (params.NUM_FRAMES, params.NUM_BANDS, 1)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        aud_input = Input(shape=input_shape, name='input_1')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            aud_input = Input(tensor=input_tensor, shape=input_shape, name='input_1')\n",
    "        else:\n",
    "            aud_input = input_tensor\n",
    "\n",
    "\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv1')(aud_input)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool1')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool2')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3/conv3_1')(x)\n",
    "    x = Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv3/conv3_2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool3')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4/conv4_1')(x)\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', name='conv4/conv4_2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same', name='pool4')(x)\n",
    "\n",
    "\n",
    "\n",
    "    if include_top:\n",
    "        # FC block\n",
    "        x = Flatten(name='flatten_')(x)\n",
    "        x = Dense(4096, activation='relu', name='vggish_fc1/fc1_1')(x)\n",
    "        x = Dense(4096, activation='relu', name='vggish_fc1/fc1_2')(x)\n",
    "        x = Dense(out_dim, activation='relu', name='vggish_fc2')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = aud_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='VGGish')\n",
    "\n",
    "\n",
    "    # load weights\n",
    "    if load_weights:\n",
    "        if weights == 'audioset':\n",
    "            if include_top:\n",
    "                model.load_weights(WEIGHTS_PATH_TOP)\n",
    "            else:\n",
    "                model.load_weights(WEIGHTS_PATH)\n",
    "        else:\n",
    "            print(\"failed to load weights\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = VGGish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs_train, labels_train, batch_size=64, epochs=1024, callbacks=callbacks, validation_data=(inputs_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = int(np.random.uniform()*len(inputs_test))\n",
    "spectrogram_out = inputs_test[ind]\n",
    "\n",
    "ipd.Audio(spectrogram_out, rate=desired_sr)\n",
    "\n",
    "y = labels_test[ind]\n",
    "output = model.predict(np.expand_dims(np.array([inputs_test[ind]]), 0))\n",
    "\n",
    "print(\"True label:\", textLabel(np.argmax(y)))\n",
    "print(\"Prediction:\", textLabel(np.argmax(output)))\n",
    "\n",
    "plt.imshow(spectrogram_out, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = spectrogramOp(np.random.random((desired_samples)))\n",
    "silence = spectrogramOp(np.zeros((desired_samples)))\n",
    "background_out, silence_out = model.predict(np.array([background, silence]))\n",
    "print(\"Predicted \", textLabel(background_out.argmax()), \"on random audio with vector\", background_out)\n",
    "print(\"Predicted \", textLabel(silence_out.argmax()), \"on null audio with vector\", silence_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
