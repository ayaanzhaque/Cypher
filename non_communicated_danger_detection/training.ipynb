{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for data.\n",
    "path = \"/kaggle/input/cremad/AudioWAV/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_directory_list = os.listdir(path)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # storing file paths\n",
    "    file_path.append(Crema + file)\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "ata_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# (-1, 126, 65)  = inputs_train.shape. Replace the numbers from inputs_train, if it changes.\n",
    "# 126 = int(desired_samples/hop_len + 1), = time axis\n",
    "# 65  = int(fft_len/2 + 1) = freq bins\n",
    "\n",
    "lambda1 = tf.keras.layers.Lambda(lambda x: tf.reshape(x, (-1, int(fft_len/2 + 1), int(desired_samples/hop_len + 1), 1)), \n",
    "                                 name=\"add_channels\", input_shape=(None, int(fft_len/2 + 1), int(desired_samples/hop_len + 1)))\n",
    "conv2d1 = tf.keras.layers.Conv2D(16, (int(fft_len/2 + 1), 4), strides=1, activation='relu', name=\"conv1\", \n",
    "                                 input_shape=(int(fft_len/2 + 1), int(desired_samples/hop_len + 1), 1))\n",
    "conv2d2 = tf.keras.layers.Conv2D(32, (1, 4), strides=4, activation='relu', name=\"conv2\")\n",
    "conv2d3 = tf.keras.layers.Conv2D(64, (1, 4), strides=3, activation='relu', name=\"conv3\")\n",
    "flatten1 = tf.keras.layers.Flatten()\n",
    "dense1  = tf.keras.layers.Dense(6*n_classes)\n",
    "dense2  = tf.keras.layers.Dense(n_classes)\n",
    "dropout = tf.keras.layers.Dropout(0.2)\n",
    "activation1 = tf.keras.layers.Activation('softmax')\n",
    "\n",
    "model.add(lambda1)\n",
    "model.add(conv2d1)\n",
    "model.add(conv2d2)\n",
    "model.add(conv2d3)\n",
    "model.add(flatten1)\n",
    "model.add(dense1)\n",
    "model.add(dense2) \n",
    "model.add(dropout)\n",
    "model.add(activation1)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'],\n",
    "              loss=(tf.keras.losses.binary_crossentropy if (n_classes==2) else tf.keras.losses.categorical_crossentropy))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs_train, labels_train, batch_size=64, epochs=1024, callbacks=callbacks, validation_data=(inputs_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = int(np.random.uniform()*len(inputs_test))\n",
    "spectrogram_out = inputs_test[ind]\n",
    "\n",
    "ipd.Audio(spectrogram_out, rate=desired_sr)\n",
    "\n",
    "y = labels_test[ind]\n",
    "output = model.predict(np.expand_dims(np.array([inputs_test[ind]]), 0))\n",
    "\n",
    "print(\"True label:\", textLabel(np.argmax(y)))\n",
    "print(\"Prediction:\", textLabel(np.argmax(output)))\n",
    "\n",
    "plt.imshow(spectrogram_out, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "# Playing the sample\n",
    "'''\n",
    "audio = librosa.core.istft(spectrogram_out, hop_length=hop_len, win_length=win_len)\n",
    "playback_sr, audio_interpolated = interpolateAudio(audio)\n",
    "ipd.Audio(audio_interpolated, rate=playback_sr, autoplay=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = spectrogramOp(np.random.random((desired_samples)))\n",
    "silence = spectrogramOp(np.zeros((desired_samples)))\n",
    "background_out, silence_out = model.predict(np.array([background, silence]))\n",
    "print(\"Predicted \", textLabel(background_out.argmax()), \"on random audio with vector\", background_out)\n",
    "print(\"Predicted \", textLabel(silence_out.argmax()), \"on null audio with vector\", silence_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
